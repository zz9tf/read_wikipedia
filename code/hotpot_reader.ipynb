{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('hotpot_train_v1.1.json', 'r') as input_file:\n",
    "    data = json.loads(input_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, bz2, json\n",
    "\n",
    "# Walk through the extracted files\n",
    "output_directory = \"../data/enwiki-20171001-pages-meta-current-withlinks-processed\"\n",
    "def load_datas():\n",
    "    datas = []\n",
    "    i = 0\n",
    "    for folder in os.listdir(output_directory):\n",
    "        for filename in os.listdir(os.path.join(output_directory, folder)):\n",
    "            if filename.endswith('.bz2'):  # Check if the file is a .bz2 file\n",
    "                file_path = os.path.join(output_directory, folder, filename)\n",
    "                # Decompress the .bz2 file\n",
    "                try:\n",
    "                    with bz2.open(file_path, 'rt') as bz2_file:  # 'rt' for text mode\n",
    "                        for line in bz2_file:\n",
    "                            data = json.loads(line)\n",
    "                            datas.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error opening file {file_path}: {e}\")\n",
    "                if i == 10:\n",
    "                    return datas\n",
    "                print(i)\n",
    "                i += 1\n",
    "    return datas\n",
    "datas = load_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas[4566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = datas[4566]\n",
    "\n",
    "for i, (paragraph, paragraph_offset) in enumerate(zip(data['text'], data['charoffset'])):\n",
    "    sentences = []\n",
    "    for sentence, sentence_list in zip(paragraph, paragraph_offset):\n",
    "        sentence = re.sub(r\"\\s*<.*?>\\s*\", \" \", sentence).strip()\n",
    "        sentences.append(sentence)\n",
    "    paragraph = ''.join(sentences)\n",
    "    print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contain_title = 0\n",
    "for document_id, data in enumerate(datas):\n",
    "    contain_title_this_doc = False\n",
    "    for i, (paragraph, paragraph_offset) in enumerate(zip(data['text'], data['charoffset'])):\n",
    "        sentences = []\n",
    "        for sentence, sentence_list in zip(paragraph, paragraph_offset):\n",
    "            sentence = re.sub(r\"\\s*<.*?>\\s*\", \" \", sentence).strip()\n",
    "            sentences.append(sentence)\n",
    "        paragraph = ''.join(sentences)\n",
    "        if len(paragraph) < 30 and len(paragraph) != 0 and paragraph != data['title']:\n",
    "            print(paragraph)\n",
    "            contain_title_this_doc = True\n",
    "    if contain_title_this_doc:\n",
    "        print(f'document_id: {document_id}')\n",
    "        contain_title += 1\n",
    "print(f\"{contain_title/len(datas)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_list in data['text']:\n",
    "    print(one_list)\n",
    "    for i, text in enumerate(one_list):\n",
    "        print(i)\n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_reader():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, bz2, json\n",
    "\n",
    "# Walk through the extracted files\n",
    "output_directory = \"../data/enwiki-20171001-pages-meta-current-withlinks-processed\"\n",
    "def load_datas():\n",
    "    datas = []\n",
    "    i = 0\n",
    "    for folder in os.listdir(output_directory):\n",
    "        for filename in os.listdir(os.path.join(output_directory, folder)):\n",
    "            if filename.endswith('.bz2'):  # Check if the file is a .bz2 file\n",
    "                file_path = os.path.join(output_directory, folder, filename)\n",
    "                # Decompress the .bz2 file\n",
    "                try:\n",
    "                    with bz2.open(file_path, 'rt') as bz2_file:  # 'rt' for text mode\n",
    "                        for line in bz2_file:\n",
    "                            data = json.loads(line)\n",
    "                            datas.append(data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON in file {file_path}: {e}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error opening file {file_path}: {e}\")\n",
    "                if i == 10:\n",
    "                    return datas\n",
    "                print(i)\n",
    "                i += 1\n",
    "    return datas\n",
    "datas = load_datas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os.path\n",
    "import re\n",
    "import argparse\n",
    "import bz2\n",
    "\n",
    "input_file_path = '../data/enwiki-latest-pages-articles.xml'\n",
    "\n",
    "if input_file_path.lower().endswith(\".bz2\"):\n",
    "    input_file = bz2.open(input_file_path, mode='rt', encoding='utf-8')\n",
    "else:\n",
    "    input_file = open(input_file_path)\n",
    "tagRE = re.compile(r'(.*?)<(/?\\w+)[^>]*>(?:([^<]*)(<.*?>)?)?')\n",
    "page = []\n",
    "for line in input_file:\n",
    "    print(line)\n",
    "    # m = tagRE.search(line)\n",
    "    # if '<' not in line:         # faster than doing re.search()\n",
    "    #         if page:\n",
    "    #             page.append(line)\n",
    "    # if m:\n",
    "    #     print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "    #     for v in m.groups():\n",
    "    #         print(f'> {v}')\n",
    "    #     print(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "    # print(\"*************************************\")\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from lxml import etree\n",
    "\n",
    "def go_over_string_element(xml_data):\n",
    "    # Parse the string as a file-like object\n",
    "    root = etree.fromstring(xml_data)\n",
    "    redirect = root.find('redirect')\n",
    "    title = root.findtext('title')\n",
    "    text_element = root.find(\".//text\")  # Find the text element (can be nested)\n",
    "    if redirect is not None or text_element is None or text_element.text is None: return None\n",
    "    text = text_element.text.strip()\n",
    "    print(text)\n",
    "    input()\n",
    "    \n",
    "    return {'title': title, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from xml.etree.ElementTree import XMLPullParser\n",
    "from lxml import etree\n",
    "\n",
    "# Create an event-driven parser\n",
    "raw_page = None\n",
    "end_page = False\n",
    "pages = []\n",
    "pages_key = {}\n",
    "input_file_path = '../data/enwiki-latest-pages-articles.xml'\n",
    "\n",
    "total_iterations = 100000\n",
    "parser = XMLPullParser(events=('start', 'end'))\n",
    "\n",
    "with tqdm(total=total_iterations, desc=\"Processing\") as pbar:\n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            parser.feed(line)\n",
    "            # print(line.strip())\n",
    "            for event, element in parser.read_events():\n",
    "                key = element.tag.split('}')[-1]\n",
    "                if event == 'start' and 'page' in element.tag:\n",
    "                    raw_page = []\n",
    "                if event == 'end' and 'page' in element.tag:\n",
    "                    end_page = True\n",
    "            # add line\n",
    "            if isinstance(raw_page, list):\n",
    "                raw_page.append(line)\n",
    "\n",
    "            # parse string when end page\n",
    "            if end_page:\n",
    "                raw_page = ''.join(raw_page)\n",
    "                page = go_over_string_element(raw_page)\n",
    "                if page:\n",
    "                    for key in page:\n",
    "                        pages_key[key] = pages_key.get(key, 0) + 1\n",
    "                    pages.append(page)\n",
    "                    pbar.set_postfix(pages_key)\n",
    "                    pbar.update(1)\n",
    "                    # input()\n",
    "                if len(pages) == total_iterations:\n",
    "                    break\n",
    "                raw_page = None\n",
    "                end_page = False\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    if page == {}:\n",
    "        print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages[:1]:\n",
    "    sections = {}\n",
    "    page_text = page['text']\n",
    "    abstract_pattern = r\"\\n'''(.*?)'''(.*?)(?=\\n\\=|$)\"\n",
    "    abstract_match = re.search(abstract_pattern, pages[0]['text'], re.DOTALL)\n",
    "    abstract = abstract_match.group(0).strip() if abstract_match else None\n",
    "    sections['abstract'] = abstract\n",
    "    \n",
    "    raw_sections = re.split(r'(?m)^==\\s*', pages[10]['text'])\n",
    "    old_section_title = None\n",
    "    for section in raw_sections[1:]:\n",
    "        lines = section.split('\\n')\n",
    "        section_title = lines[0].strip('= ')\n",
    "        section_content = \"\\n\".join(lines[1:])\n",
    "        if not section.startswith('='):\n",
    "            sections[section_title] = section_content\n",
    "            old_section_title = section_title\n",
    "        else:\n",
    "            assert old_section_title is not None, 'old_section_title shouldn\\'t be None'\n",
    "            sections[old_section_title] += f'\\nsub title: {section_title}\\n{section_content}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sections.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _clean_text(raw_text):\n",
    "    print(raw_text)\n",
    "    cleaned_text = re.sub(r'\\{\\{.*?\\}\\}', '', raw_text, flags=re.DOTALL)\n",
    "    print(cleaned_text)\n",
    "    cleaned_text = re.sub(r\"'''(.*?)'''\", r'\\1', cleaned_text)\n",
    "    print(cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[\\[(?:[^\\]|]+\\|)?([^\\]]+)\\]\\]', r'\\1', cleaned_text)\n",
    "    print(cleaned_text)\n",
    "    input()\n",
    "    return cleaned_text\n",
    "\n",
    "def _read_page(page):\n",
    "    sections = {}\n",
    "    page_text = page['text']\n",
    "    abstract_pattern = r\"\\n'''(.*?)'''(.*?)(?=\\n\\=|$)\"\n",
    "    abstract_match = re.search(abstract_pattern, page_text, re.DOTALL)\n",
    "    abstract = abstract_match.group(0).strip() if abstract_match else None\n",
    "    sections['abstract'] = _clean_text(abstract)\n",
    "    \n",
    "    raw_sections = re.split(r'(?m)^==\\s*', page_text)\n",
    "    old_section_title = None\n",
    "    for section in raw_sections[1:]:\n",
    "        lines = section.split('\\n')\n",
    "        section_title = lines[0].strip('= ')\n",
    "        section_content = _clean_text(\"\\n\".join(lines[1:]))\n",
    "        if not section.startswith('='):\n",
    "            sections[section_title] = section_content\n",
    "            old_section_title = section_title\n",
    "        else:\n",
    "            assert old_section_title is not None, 'old_section_title shouldn\\'t be None'\n",
    "            sections[old_section_title] += f'\\nsub title: {section_title}\\n{section_content}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sections.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "short_descriptions = []\n",
    "abstracts = []\n",
    "sections = []\n",
    "sections_of_one_document = []\n",
    "\n",
    "for text in pages[1]['text'].split('\\n'):\n",
    "    if 'short description' in text.lower():\n",
    "        result = re.search(r'{{\\s*short description\\s*\\|([^}]+)}}', text, re.IGNORECASE)\n",
    "        if result:\n",
    "            short_description = result.group(1).strip()  # Extract the description\n",
    "            short_descriptions.append(short_description)\n",
    "            print(short_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    if 'redirect' in page:\n",
    "        for k, v in page.items():\n",
    "            if isinstance(v, str):\n",
    "                print(v)\n",
    "            else:\n",
    "                print(f'{k}, text: {v.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of the wikipedia dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw_page_0.jsonl:   0%|          | 29.4k/6.99G [00:00<15:20, 8.16MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from string_preprocessing import _go_over_string_element, remove_self_closing_ref, remove_ref_with_content, remove_braces_content, remove_square_brackets, _clean_text, _get_abstract, _read_page\n",
    "\n",
    "titles = {}\n",
    "length = []\n",
    "section_titles = {}\n",
    "\n",
    "data_dir = '/home/zhengzheng/scratch0/projects/Fine-Tuned-GPT-2-with-articles-ground-truth-main/code/llamaIndex/.cache'\n",
    "filenames = sorted(list(os.listdir(data_dir)), key=lambda x: x.split('_')[-1].split('.')[0])\n",
    "save_file = open('temp.txt', 'w')\n",
    "for filename in filenames[:1]:\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    with open(file_path, 'r') as input_file:\n",
    "        with tqdm(total=file_size, desc=f'Loading {file_path.split(os.path.sep)[-1]}', unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "            for i, line in enumerate(input_file):\n",
    "                if i < 71:\n",
    "                    continue\n",
    "                pbar.update(len(line))\n",
    "                data = json.loads(line)\n",
    "                page = _go_over_string_element(data['page'])\n",
    "                if page:\n",
    "                    save_file.write(page['text'])\n",
    "                    save_file.flush()\n",
    "                    break\n",
    "                    paper_content, file_dict, print_str = _read_page(page)\n",
    "                    if paper_content:\n",
    "                        save_file.write(page['text'] + '\\n=====================================\\n' + paper_content)\n",
    "                        save_file.flush()\n",
    "                        # titles[file_dict['title']] = titles.get(file_dict['title'], 0) + 1\n",
    "                        # length.append(len(paper_content))\n",
    "                        # for key in file_dict['sections']:\n",
    "                        #     section_titles[key] = section_titles.get(key, 0) + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An important relationship between an object\\'s astronomical (geometric) albedo, absolute magnitude and diameter is given by:\\n<math display=\"block\">A =\\\\left ( \\\\frac{1329\\\\times10^{-H/5}}{D} \\\\right ) ^2,</math>\\nwhere <math>A</math> is the astronomical albedo, <math>D</math> is the diameter in kilometers, and <math>H</math> is the absolute magnitude.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "from string_preprocessing import _go_over_string_element, remove_self_closing_ref, remove_ref_with_content, remove_braces_content, remove_square_brackets, _clean_text, _get_abstract, _read_page\n",
    "\n",
    "text = r\"\"\"An important relationship between an object's astronomical (geometric) albedo, [[Absolute magnitude#Absolute magnitude for planets (H)|absolute magnitude]] and diameter is given by:<ref name=\"bruton\">{{cite web\n",
    " |title=Conversion of Absolute Magnitude to Diameter for Minor Planets\n",
    " |publisher=Department of Physics & Astronomy (Stephen F. Austin State University)\n",
    " |author=Dan Bruton\n",
    " |url=http://www.physics.sfasu.edu/astro/asteroids/sizemagnitude.html\n",
    " |access-date=7 October 2008\n",
    " |archive-url=https://web.archive.org/web/20081210190134/http://www.physics.sfasu.edu/astro/asteroids/sizemagnitude.html\n",
    " |archive-date=10 December 2008\n",
    " |url-status=dead\n",
    "}}</ref>\n",
    "<math display=\"block\">A =\\left ( \\frac{1329\\times10^{-H/5}}{D} \\right ) ^2,</math>\n",
    "where <math>A</math> is the astronomical albedo, <math>D</math> is the diameter in kilometers, and <math>H</math> is the absolute magnitude.\n",
    "\"\"\"\n",
    "\n",
    "# cleaned_text = html.unescape(text)\n",
    "# # # Remove <ref> tag without greedy\n",
    "# cleaned_text = remove_self_closing_ref(cleaned_text)\n",
    "# # print(cleaned_text)\n",
    "# cleaned_text = remove_ref_with_content(cleaned_text)\n",
    "# print(cleaned_text)\n",
    "\n",
    "# stack = []\n",
    "# result = []\n",
    "# i = 0\n",
    "# text = cleaned_text\n",
    "\n",
    "# while i < len(text):\n",
    "#     char = text[i]\n",
    "#     if char == '{' and i + 1 < len(text) and text[i + 1] == '{':\n",
    "#         # Push the current length of result onto the stack to remember where the brace started\n",
    "#         stack.append(len(result))\n",
    "#         i += 1  # Skip the second '{'\n",
    "#     elif char == '}' and i + 1 < len(text) and text[i + 1] == '}' and len(stack) > 0:\n",
    "#         print(''.join(result))\n",
    "#         print()\n",
    "#         print(len(stack))\n",
    "#         print(\"\\n==========================================\\n\\n\")\n",
    "        \n",
    "#         start = stack.pop()\n",
    "#         block_content = ''.join(result[start:])\n",
    "#         if '|' in block_content and block_content[0] == 'blackquate' and len(block_content.split('|')) == 2:\n",
    "#             keep_content = block_content.split('|')[1]\n",
    "#             result = result[:start] + list(keep_content)\n",
    "#         else:\n",
    "#             result = result[:start]\n",
    "#         i += 1  # Skip the second '}'\n",
    "#     else:\n",
    "#         # Append character to the result only if not within braces\n",
    "#         result.append(char)\n",
    "#     i += 1\n",
    "# print(''.join(result))\n",
    "\n",
    "# stack = []\n",
    "# result = []\n",
    "# for char in cleaned_text:\n",
    "#     if char == '{':\n",
    "#         # Push the current length of result onto the stack to remember where the brace started\n",
    "#         stack.append(len(result))\n",
    "#     elif char == '}':\n",
    "#         print(\"get }\")\n",
    "#         print(''.join(result))\n",
    "#         start = stack.pop()\n",
    "#     else:\n",
    "#         # Append character to the result only if not within braces\n",
    "#         if len(stack) == 0:\n",
    "#             result.append(char)\n",
    "\n",
    "_clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch0/zhengzheng/projects/read_wikipedia/code\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./processed_data.json', 'r') as input_file:\n",
    "    output = json.loads(input_file.read())\n",
    "    titles = output['titles']\n",
    "    length = output['length']\n",
    "    section_titles = output['section_titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for title in titles:\n",
    "    if title.lower().startswith('file:'):\n",
    "        print(title)\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in sorted(list(section_titles.items()), key=lambda x: x[1], reverse=True)[:30]:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(length, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a violin plot\n",
    "sns.violinplot(y=length, color='skyblue', inner='box')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Numbers')\n",
    "plt.title('Violin Plot of Number Distribution')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dictionary\n",
    "my_dict = {\n",
    "    'references': 241830,\n",
    "    'External links': 182700,\n",
    "    'See also': 138906,\n",
    "    'Further reading': 40252,\n",
    "    'Introduction': 12345,\n",
    "}\n",
    "\n",
    "# Keys to remove\n",
    "keys_to_remove = ['References', 'External links', 'See also', 'Further reading']\n",
    "\n",
    "# Remove keys safely\n",
    "for key in keys_to_remove:\n",
    "    my_dict.pop(key, None)  # None is the default value to avoid errors if the key doesn't exist\n",
    "\n",
    "# Output the modified dictionary\n",
    "print(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
